{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom training: basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "      pass\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Tensors in TensorFlow are immutable stateless objects. Machine learning models, however, must have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!). To represent this state, which needs to change over the course of your computation, you can choose to rely on the fact that Python is a stateful programming language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using python state\n",
    "x = tf.zeros([10,10])\n",
    "x += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has stateful operations built-in, and these are often easier than using low-level Python representations for your state. Use `tf.Variable` to represent weights in a model.\n",
    "\n",
    "A `tf.Variable` object stores a value and implicitly reads from this stored value. There are operations (`tf.assign_sub`, `tf.scatter_update`, etc.) that manipulate the value stored in a TensorFlow variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable(1.0)\n",
    "# Use python assert as a debugging statement to test the condition\n",
    "assert v.numpy()==1.0\n",
    "\n",
    "# Reassign the value of v\n",
    "v.assign(2.0)\n",
    "assert v.numpy() == 2.0\n",
    "\n",
    "# Use `v` in a TensorFlow `tf.square()` operation and reassign\n",
    "v.assign(tf.square(v))\n",
    "assert v.numpy() == 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations using `tf.Variable` are automatically traced when computing gradients. For variables that represent embeddings, TensorFlow will do sparse updates by default, which are more computation and memory efficient.\n",
    "\n",
    "A `tf.Variable` is also a way to show a reader of your code that a piece of state is mutable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a linear model\n",
    "\n",
    "Let's use the concepts you have learned so far—`Tensor`, `Variable`, and `GradientTape`—to build and train a simple model. This typically involves a few steps:\n",
    "\n",
    "1. Define the model.\n",
    "2. Define a loss function.\n",
    "3. Obtain training data.\n",
    "4. Run through the training data and use an \"optimizer\" to adjust the variables to fit the data.\n",
    "\n",
    "Here, you'll create a simple linear model, `f(x) = x * W + b`, which has two variables: `W` (weights) and `b` (bias). You'll synthesize data such that a well trained model would have `W = 3.0` and `b = 2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "Let's define a simple class to encapsulate the variables and the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(0.0)\n",
    "    def __call__(self,x):\n",
    "        return self.W*x+self.b\n",
    "\n",
    "model = Model()\n",
    "assert model(3.0).numpy() == 15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a loss function\n",
    "\n",
    "A loss function measures how well the output of a model for a given input matches the target output. The goal is to minimize this difference during training. Let's use the standard L2 loss, also known as the least square errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(predicted_y,target_y):\n",
    "    return tf.reduce_mean(tf.square(predicted_y-target_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain training data\n",
    "\n",
    "First, synthesize the training data by adding random Gaussian (Normal) noise to the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_W = 3.0\n",
    "TRUE_b = 2.0\n",
    "NUM_EXAMPLES = 1000\n",
    "inputs = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "outputs = inputs*TRUE_W+TRUE_b+noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, visualize the loss value by plotting the model's predictions in red and the training data in blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 9.560630\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(figsize=(10,7))\n",
    "plt.scatter(inputs, outputs, c='b')\n",
    "plt.scatter(inputs, model(inputs), c='r')\n",
    "plt.show()\n",
    "\n",
    "print('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a training loop\n",
    "\n",
    "With the network and training data, train the model using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to update the weights variable (`W`) and the bias variable (`b`) to reduce the loss. There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer`—our recommended implementation. But in the spirit of building from first principles, here you will implement the basic math yourself with the help of `tf.GradientTape` for automatic differentiation and `tf.assign_sub` for decrementing a value (which combines `tf.assign` and `tf.sub`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,inputs,outputs,learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(model(inputs),outputs)\n",
    "    dw,db = t.gradient(current_loss,[model.W,model.b])\n",
    "    model.W.assign_sub(learning_rate*dw)\n",
    "    model.b.assign_sub(learning_rate*db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W.numpy()\n",
    "model.b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's repeatedly run through the training data and see how `W` and `b` evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: W=5.00 b=0.00, loss=9.56063\n",
      "Epoch  1: W=4.96 b=0.04, loss=9.20934\n",
      "Epoch  2: W=4.92 b=0.08, loss=8.87243\n",
      "Epoch  3: W=4.88 b=0.13, loss=8.54930\n",
      "Epoch  4: W=4.84 b=0.17, loss=8.23939\n",
      "Epoch  5: W=4.80 b=0.21, loss=7.94216\n",
      "Epoch  6: W=4.76 b=0.24, loss=7.65709\n",
      "Epoch  7: W=4.73 b=0.28, loss=7.38368\n",
      "Epoch  8: W=4.69 b=0.32, loss=7.12146\n",
      "Epoch  9: W=4.66 b=0.36, loss=6.86996\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model()\n",
    "#collect the history of w-values and b-values to plot later\n",
    "ws,bs = [],[]\n",
    "epochs = range(10)\n",
    "for epoch in epochs:\n",
    "    ws.append(model.W.numpy())\n",
    "    bs.append(model.b.numpy())\n",
    "    current_loss = loss(model(inputs), outputs)\n",
    "    \n",
    "    train(model,inputs,outputs,learning_rate = 0.01)\n",
    "    print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n",
    "        (epoch, ws[-1], bs[-1], current_loss))\n",
    "    \n",
    "# Let's plot it all\n",
    "plt.plot(epochs, ws, 'r',\n",
    "         epochs, bs, 'b')\n",
    "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
    "         [TRUE_b] * len(epochs), 'b--')\n",
    "plt.legend(['W', 'b', 'True W', 'True b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
